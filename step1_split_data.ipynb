{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fathHzuEgx8_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import shuffle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def read_excel_data(excel_path):\n",
        "\n",
        "    df = pd.read_excel(excel_path)\n",
        "    header_name_list = list(df.head())\n",
        "    del header_name_list[0]\n",
        "    \n",
        "    all_data = np.array(df)\n",
        "    all_data = np.delete(all_data, 0, axis=1)\n",
        "    all_data = np.concatenate([np.reshape(header_name_list, (1, -1)), all_data], axis=0)\n",
        "    print(len(header_name_list), header_name_list)\n",
        "    print('read all_data shape: {}'.format(np.shape(all_data)))\n",
        "\n",
        "\n",
        "    row, col = np.shape(all_data)\n",
        "    source_name_list, header_name_list, label_list, data = [], [], [], []\n",
        "\n",
        "    for i in range(row):\n",
        "        row_data = []\n",
        "        for j in range(col):\n",
        "            if i == 0:\n",
        "                if 'source_name' == all_data[i, j]:\n",
        "                    source_name_index = j\n",
        "                    print('source_name_index: {}'.format(source_name_index))\n",
        "                    continue\n",
        "                elif 'label' == all_data[i, j]:\n",
        "                    label_name_index = j\n",
        "                    print('label_name_index: {}'.format(label_name_index))\n",
        "                    continue\n",
        "                header_name_list.append(all_data[i, j])\n",
        "            else:\n",
        "                if source_name_index == j:\n",
        "                    source_name_list.append(all_data[i, j])\n",
        "                elif label_name_index == j:\n",
        "                    label_list.append(all_data[i, j])\n",
        "                else:\n",
        "                    try:\n",
        "                        row_data.append(float(all_data[i, j]))\n",
        "                    except:\n",
        "                        print('error in ({}, {}) -> {}'.format(i, j, all_data[i, j]))\n",
        "        if len(row_data) > 0:\n",
        "            data.append(row_data)\n",
        "\n",
        "    return source_name_list, header_name_list, label_list, np.array(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18 ['source_name', 'label', 'Pivot_Energy', 'Flux1000', 'Unc_Flux1000', 'PL_Index', 'Unc_PL_Index', 'Variability_Index', 'Frac_Variability', 'Unc_Frac_Variability', 'Flux_Band1', 'Flux_Band2', 'Flux_Band3', 'Flux_Band4', 'Flux_Band5', 'Flux_Band6', 'Flux_Band7', 'Flux_Band8']\n",
            "read all_data shape: (6660, 18)\n",
            "source_name_index: 0\n",
            "label_name_index: 1\n",
            "agn 0\n",
            "non-agn 1\n",
            "shape of predict samples (2291, 16)\n",
            "data 1: (3809, 16) data 2: (559, 16)\n",
            "split 1, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 2, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 3, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 4, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 5, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 6, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 7, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 8, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 9, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "split 10, train: (3494, 16), validation: (435, 16), test: (439, 16)\n",
            "18 ['source_name', 'label', 'Pivot_Energy', 'Flux1000', 'Unc_Flux1000', 'PL_Index', 'Unc_PL_Index', 'Variability_Index', 'Frac_Variability', 'Unc_Frac_Variability', 'Flux_Band1', 'Flux_Band2', 'Flux_Band3', 'Flux_Band4', 'Flux_Band5', 'Flux_Band6', 'Flux_Band7', 'Flux_Band8']\n",
            "read all_data shape: (3744, 18)\n",
            "source_name_index: 0\n",
            "label_name_index: 1\n",
            "bll 0\n",
            "fsrq 1\n",
            "shape of predict samples (1493, 16)\n",
            "data 1: (1456, 16) data 2: (794, 16)\n",
            "split 1, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 2, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 3, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 4, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 5, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 6, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 7, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 8, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 9, train: (1799, 16), validation: (224, 16), test: (227, 16)\n",
            "split 10, train: (1799, 16), validation: (224, 16), test: (227, 16)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "split_info = [ # [file_path, label_str_1, label_str_2, label_predict, save_npy_base_dir]\n",
        "    [r'datasets\\Dataset_A.xlsx', 'agn', 'non-agn', 'un', r'datasets/NPY_DATA_A'],\n",
        "    [r'datasets\\Dataset_B.xlsx', 'bll', 'fsrq', 'bcu', r'datasets/NPY_DATA_B']\n",
        "]\n",
        "train_ratio, validation_ratio = 0.8, 0.1\n",
        "split_repeat = 10 # randomly split 10 times\n",
        "\n",
        "\n",
        "for file_path, label_str_1, label_str_2, label_predict, save_npy_base_dir in split_info:\n",
        "    os.makedirs(save_npy_base_dir, exist_ok=True)\n",
        "\n",
        "    # load excel data\n",
        "    source_name_list, header_name_list, label_list, data = read_excel_data(file_path)\n",
        "\n",
        "    # generate the dict of the label to index\n",
        "    label2index, index2label = {}, {}\n",
        "    cnt = 0\n",
        "    for lab in label_list:\n",
        "        if lab not in label2index.keys() and lab != label_predict:\n",
        "            label2index[lab] = cnt\n",
        "            index2label[cnt] = lab\n",
        "            cnt += 1\n",
        "    for lab, indx in label2index.items():\n",
        "        print(lab, indx)\n",
        "    row, col = np.shape(data)\n",
        "    assert len(source_name_list) == row\n",
        "    assert len(header_name_list) == col\n",
        "    assert len(label_list) == row\n",
        "\n",
        "    # save the predict samples\n",
        "    predict_sourcename_list, predict_data_list = [], []\n",
        "    for i in range(len(source_name_list)):\n",
        "        source_name = source_name_list[i]\n",
        "        label = label_list[i]\n",
        "        if label == label_predict:\n",
        "            predict_sourcename_list.append(source_name)\n",
        "            predict_data_list.append(data[i])\n",
        "    predict_data_list = np.array(predict_data_list)\n",
        "    print('shape of predict samples', np.shape(predict_data_list))\n",
        "    np.save(os.path.join(save_npy_base_dir, 'predict_data.npy'), predict_data_list)\n",
        "    np.save(os.path.join(save_npy_base_dir, 'predict_sourcename.npy'), predict_sourcename_list)\n",
        "    np.save(os.path.join(save_npy_base_dir, 'header_name.npy'), header_name_list)\n",
        "\n",
        "\n",
        "    # randomly split the samples many times\n",
        "    source_name_list_1, source_name_list_2 = [], []\n",
        "    label_list_1, label_list_2 = [], []\n",
        "    attri_data_list_1, attri_data_list_2 = [], []\n",
        "    for i in range(len(source_name_list)):\n",
        "        source_name = source_name_list[i]\n",
        "        indx = source_name_list.index(source_name)\n",
        "        label = label_list[indx]\n",
        "        if label == label_predict:\n",
        "            continue\n",
        "        label_int = label2index[label]\n",
        "        attri_data = data[indx]\n",
        "        if label == label_str_1:\n",
        "            source_name_list_1.append(source_name)\n",
        "            label_list_1.append(label_int)\n",
        "            attri_data_list_1.append(attri_data)\n",
        "        elif label == label_str_2:\n",
        "            source_name_list_2.append(source_name)\n",
        "            label_list_2.append(label_int)\n",
        "            attri_data_list_2.append(attri_data)\n",
        "        else:\n",
        "            print('label error:', label)\n",
        "            exit()\n",
        "    source_name_list_1 = np.array(source_name_list_1)\n",
        "    label_list_1 = np.array(label_list_1)\n",
        "    attri_data_list_1 = np.array(attri_data_list_1)\n",
        "    source_name_list_2 = np.array(source_name_list_2)\n",
        "    label_list_2 = np.array(label_list_2)\n",
        "    attri_data_list_2 = np.array(attri_data_list_2)\n",
        "    print('data 1:', np.shape(attri_data_list_1), 'data 2:', np.shape(attri_data_list_2))\n",
        "    num_samples_1 = len(source_name_list_1)\n",
        "    num_samples_2 = len(source_name_list_2)\n",
        "    num_train_1 = int(train_ratio*num_samples_1)\n",
        "    num_validation_1 = int(validation_ratio*num_samples_1)\n",
        "    num_train_2 = int(train_ratio*num_samples_2)\n",
        "    num_validation_2 = int(validation_ratio*num_samples_2)\n",
        "\n",
        "\n",
        "    # randomly split\n",
        "    for split_num in range(1, split_repeat+1):\n",
        "        index_list_1 = [i for i in range(num_samples_1)]\n",
        "        index_list_2 = [i for i in range(num_samples_2)]\n",
        "        shuffle(index_list_1)\n",
        "        shuffle(index_list_2)\n",
        "\n",
        "        train_sourcename_list, train_data, train_labels = [], [], []\n",
        "        validation_sourcename_list, validation_data, validation_labels = [], [], []\n",
        "        test_sourcename_list, test_data, test_labels = [], [], []\n",
        "\n",
        "        for i in range(num_samples_1):\n",
        "            indx = index_list_1[i]\n",
        "            source_name = source_name_list_1[indx]\n",
        "            attri_data = attri_data_list_1[indx]\n",
        "            label = label_list_1[indx]\n",
        "            if i < num_train_1:\n",
        "                train_sourcename_list.append(source_name)\n",
        "                train_data.append(attri_data)\n",
        "                train_labels.append(label)\n",
        "            elif i >= num_train_1 and i < num_train_1+num_validation_1:\n",
        "                validation_sourcename_list.append(source_name)\n",
        "                validation_data.append(attri_data)\n",
        "                validation_labels.append(label)\n",
        "            else:\n",
        "                test_sourcename_list.append(source_name)\n",
        "                test_data.append(attri_data)\n",
        "                test_labels.append(label)\n",
        "\n",
        "        for i in range(num_samples_2):\n",
        "            indx = index_list_2[i]\n",
        "            source_name = source_name_list_2[indx]\n",
        "            attri_data = attri_data_list_2[indx]\n",
        "            label = label_list_2[indx]\n",
        "            if i < num_train_2:\n",
        "                train_sourcename_list.append(source_name)\n",
        "                train_data.append(attri_data)\n",
        "                train_labels.append(label)\n",
        "            elif i >= num_train_2 and i < num_train_2+num_validation_2:\n",
        "                validation_sourcename_list.append(source_name)\n",
        "                validation_data.append(attri_data)\n",
        "                validation_labels.append(label)\n",
        "            else:\n",
        "                test_sourcename_list.append(source_name)\n",
        "                test_data.append(attri_data)\n",
        "                test_labels.append(label)\n",
        "\n",
        "\n",
        "        train_data = np.array(train_data)\n",
        "        validation_data = np.array(validation_data)\n",
        "        test_data = np.array(test_data)\n",
        "        print('split {}, train: {}, validation: {}, test: {}'.format(split_num, np.shape(train_data), np.shape(validation_data), np.shape(test_data)))\n",
        "\n",
        "        # rand_indx = np.random.randint(len(train_data))\n",
        "        # print('check train:', train_sourcename_list[rand_indx], train_labels[rand_indx], train_data[rand_indx])\n",
        "        # rand_indx = np.random.randint(len(validation_data))\n",
        "        # print('check validation:', validation_sourcename_list[rand_indx], validation_labels[rand_indx], validation_data[rand_indx])\n",
        "        # rand_indx = np.random.randint(len(test_data))\n",
        "        # print('check test:', test_sourcename_list[rand_indx], test_labels[rand_indx], test_data[rand_indx])\n",
        "\n",
        "        save_npy_dir = os.path.join(save_npy_base_dir, 'split_'+str(split_num))\n",
        "        os.makedirs(save_npy_dir, exist_ok=True)\n",
        "\n",
        "        np.save(os.path.join(save_npy_dir, 'train_data.npy'), train_data)\n",
        "        np.save(os.path.join(save_npy_dir, 'validation_data.npy'), validation_data)\n",
        "        np.save(os.path.join(save_npy_dir, 'test_data.npy'), test_data)\n",
        "        np.save(os.path.join(save_npy_dir, 'train_sourcename.npy'), train_sourcename_list)\n",
        "        np.save(os.path.join(save_npy_dir, 'validation_sourcename.npy'), validation_sourcename_list)\n",
        "        np.save(os.path.join(save_npy_dir, 'test_sourcename.npy'), test_sourcename_list)\n",
        "        np.save(os.path.join(save_npy_dir, 'train_labels.npy'), train_labels)\n",
        "        np.save(os.path.join(save_npy_dir, 'validation_labels.npy'), validation_labels)\n",
        "        np.save(os.path.join(save_npy_dir, 'test_labels.npy'), test_labels)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "01_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.16 ('tf29')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "00f10844da1a03447938456ee73c08f72d39e9b8baab80c01d0948b98bbe2238"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
